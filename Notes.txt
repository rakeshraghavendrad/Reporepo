from deepeval.metrics import GEvalMetric
from deepeval.test_case import LLMTestCase
from deepeval import evaluate

# 🧠 Create a custom G-Eval metric
metric = GEvalMetric(
    name="Helpfulness Score",
    criteria="Is the answer helpful and clear based on the input question?",
    evaluation_prompt="Given the input and output, evaluate how helpful the output is. Return JSON: {\"score\": float (0 to 1), \"reason\": string}",
    model="gpt-4",
    threshold=0.7,
    include_reason=True
)

# ✅ Test Case to be evaluated
test_case = LLMTestCase(
    input="How can I return the shoes I ordered?",
    actual_output="You can return the shoes within 30 days with no extra charges."
)

# 🔍 Run the evaluation
evaluate(test_cases=[test_case], metrics=[metric])
