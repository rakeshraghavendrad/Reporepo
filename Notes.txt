from deepeval.metrics import FaithfulnessMetric
from deepeval.test_case import LLMTestCase

# âœ… Override FaithfulnessMetric to control the prompt
class PatchedFaithfulnessMetric(FaithfulnessMetric):
    def _construct_prompt(self, context, output):
        return f"""
Evaluate how faithful the following output is to the given context.

Return your response in strict JSON format only:
{{
  "score": float (between 0 and 1),
  "reason": string
}}

Context:
"{context}"

Output:
"{output}"
"""

# âœ… Your inputs
actual_output = "We offer a 30-day full refund at no extra cost."
retrieval_context = ["All customers are eligible for a 30 day full refund at no extra cost."]

# âœ… Instantiate the patched metric
metric = PatchedFaithfulnessMetric(
    threshold=0.7,
    model="gemini-2.0-flash-001",  # Your model
    include_reason=True
)

# âœ… Create the test case
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    actual_output=actual_output,
    retrieval_context=retrieval_context
)

# âœ… Run the test and safely handle failures
try:
    metric.measure(test_case)
    print("âœ… Score:", getattr(metric, 'score', 'â“ Not Set'))
    print("ğŸ’¬ Reason:", getattr(metric, 'reason', 'â“ Not Set'))
except Exception as e:
    print("âŒ Model output could not be parsed.")
    print("ğŸ” Error:", e)
    if hasattr(e, 'args') and len(e.args) > 0:
        print("ğŸ“¤ Raw model response:\n", e.args[0])
