Correctness (0.52): Dev runs get facts right only about half the time.
→ Risk of factual errors remains significant.

Semantic Similarity (0.84): Dev runs are usually on the same topic/idea as prod.
→ They rarely go completely off-track.

ROUGE (0.39–0.49): Word overlap is low.
→ Dev runs phrase answers differently, which lowers consistency.

BERTScore (0.71): Despite low word overlap, the underlying meaning matches well.
→ Shows dev runs are semantically aligned even if they “say it differently.”

Overall:

Dev runs sound right and stay on-topic, but factual correctness is only ~52%, which is the main weakness.

If your use case requires strict accuracy, dev runs need refinement.

If semantic variety and flexibility matter more (creative tasks, brainstorming), current results are fairly good.
