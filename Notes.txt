from deepeval.metrics.contextual_precision.template import ContextualPrecisionTemplate

class CustomPrecisionTemplate(ContextualPrecisionTemplate):
    """
    Overrides only the prompt logic to return score + reason JSON
    instead of default verdicts.
    """
    @staticmethod
    def generate_verdicts(input: str, expected_output: str, retrieval_context: list):
        # Single unified prompt
        return f"""
Evaluate how contextually precise the 'actual output' is compared to the 'expected output' 
using the given retrieval context.

Return strictly in JSON format:
{{
  "score": <float between 0 and 1>,
  "reason": "<concise explanation>"
}}
Score should reflect contextual precision (1 = fully precise, 0 = not precise).

Expected Output:
{expected_output}

Actual Output:
{input}

Context:
{retrieval_context}
"""

    @staticmethod
    def generate_reason(input: str, verdicts: list, score: str):
        # Return reason directly from previous JSON
        return f"Reason: {verdicts[0]['reasons'] if verdicts else 'No reason provided'}"



from deepeval.metrics import ContextualPrecisionMetric

metric = ContextualPrecisionMetric(
    threshold=0.7,
    model=llm_no_ssl,  # your CustomOpenAILLM
    evaluation_template=CustomPrecisionTemplate
)


import pandas as pd
import json
from deepeval.test_case import LLMTestCase

def apply_ContextualPrecisionMetric(row):
    test_case = LLMTestCase(
        input="Evaluate contextual precision and return score + reason",
        actual_output=row["NewSummary"],
        expected_output=row["Agentnotes"],
        retrieval_context=[row["Agentcontext"]]
    )

    # Measure returns score (float)
    score = metric.measure(test_case)
    reason = metric.reason  # populated internally

    return pd.Series({
        "ContextualPrecisionMetric_score": score,
        "ContextualPrecisionMetric_reason": reason
    })

# Example usage
df[["ContextualPrecisionMetric_score", "ContextualPrecisionMetric_reason"]] = df.apply(
    apply_ContextualPrecisionMetric, axis=1
)
