# Override prompt template to force score + reason output
ContextualPrecisionMetric_metric.prompt_template = """
You are evaluating contextual precision between the expected output and the actual output.

Return strictly in JSON format:
{
  "score": <float between 0 and 1>,
  "reason": "<brief explanation of why this score>"
}
"""
