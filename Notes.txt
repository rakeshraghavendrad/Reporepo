
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMEvalTestCase

# Initialize the metric
answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=llm_no_ssl)

# Function to compute relevance between evidence_run1 and evidence_run2
def compare_evidence(row):
    test_case = LLMEvalTestCase(
        input=row['evidence_run1'],       # This acts as the reference
        actual_output=row['evidence_run2']  # This is what weâ€™re evaluating for relevance
    )
    answer_relevancy_metric.measure(test_case)
    return pd.Series({
        "relevance_score": answer_relevancy_metric.score,
        "relevance_reason": answer_relevancy_metric.reason
    })

# Apply row-wise comparison
df_evidence[['relevance_score', 'relevance_reason']] = df_evidence.apply(compare_evidence, axis=1)

# View the result
df_evidence.head()











