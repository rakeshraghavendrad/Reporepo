from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from deepeval.metrics import FaithfulnessMetric
import json
import re
import types

# Monkey-patch the model's _generate method to handle non-JSON responses safely
from deepeval.models.custom_openai_model import CustomOpenAIModel

def patched_generate(self, prompt, schema=None):
    response_text = self.model.generate_content(prompt).text
    clean_text = re.sub(r"```json|```", "", response_text).strip()
    try:
        json_result = json.loads(clean_text)
    except json.JSONDecodeError:
        raise ValueError(f"Model output is not valid JSON:\n{response_text}")
    return schema(**json_result)

CustomOpenAIModel._generate = types.MethodType(patched_generate, CustomOpenAIModel)

# Replace this with your actual output from the LLM
actual_output = "We offer a 30-day full refund at no extra cost."

# Replace this with your actual retrieved context from the RAG pipeline
retrieval_context = ["All customers are eligible for a 30 day full refund at no extra cost."]

# Set up the metric
metric = FaithfulnessMetric(
    threshold=0.7,
    model="llm-no-ssl",  # or your model id, like "gemini-2.0-flash-001"
    include_reason=True
)

# Define the test case
test_case = LLMTestCase(
    input="What if these shoes donâ€™t fit?",
    actual_output=actual_output,
    retrieval_context=retrieval_context
)

# Evaluate
evaluate(test_cases=[test_case], metrics=[metric])
