from pydantic import BaseModel

class ContextualPrecisionScore(BaseModel):
    score: float
    reason: str


template.py 


from typing import List

class ContextualPrecisionTemplate:
    @staticmethod
    def generate_score(input: str, expected_output: str, retrieval_context: List[str]):
        document_count_str = f" ({len(retrieval_context)} document{'s' if len(retrieval_context) > 1 else ''})"
        return f"""Given the input, expected output, and retrieval context{document_count_str},
generate a contextual precision SCORE between 0 and 1 and provide a REASON for it.

- Score meaning:
    1.0 = retrieval context perfectly matches expected output
    0.0 = completely irrelevant
    Values in between represent partial relevance.

Return strictly in JSON format:
{{
  "score": <float between 0 and 1>,
  "reason": "<concise reason using quotes from context>"
}}

Input:
{input}

Expected Output:
{expected_output}

Retrieval Context:
{retrieval_context}

JSON:
"""




precision.py 



from deepeval.metrics.contextual_precision.schema import ContextualPrecisionScore

class ContextualPrecisionMetric(BaseMetric):
    _required_params = [
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.RETRIEVAL_CONTEXT,
        LLMTestCaseParams.EXPECTED_OUTPUT,
    ]

    def measure(self, test_case: LLMTestCase, _show_indicator: bool = True, _in_component: bool = False) -> float:
        check_llm_test_case_params(test_case, self._required_params, self)
        self.evaluation_cost = 0 if self.using_native_model else None

        with metric_progress_indicator(self, _show_indicator=_show_indicator, _in_component=_in_component):
            prompt = self.evaluation_template.generate_score(
                test_case.input,
                test_case.expected_output,
                test_case.retrieval_context
            )

            if self.using_native_model:
                res, cost = self.model.generate(prompt, schema=ContextualPrecisionScore)
                self.evaluation_cost += cost
                self.score = res.score
                self.reason = res.reason
            else:
                try:
                    res: ContextualPrecisionScore = self.model.generate(prompt, schema=ContextualPrecisionScore)
                    self.score = res.score
                    self.reason = res.reason
                except TypeError:
                    res = self.model.generate(prompt)
                    data = trimAndLoadJson(res, self)
                    self.score = data["score"]
                    self.reason = data["reason"]

            self.success = self.score >= self.threshold
            self.verbose_logs = construct_verbose_logs(
                self,
                steps=[f"Score: {self.score}\nReason: {self.reason}"]
            )
            return self.score

    async def a_measure(self, test_case: LLMTestCase, _show_indicator: bool = True, _in_component: bool = False) -> float:
        check_llm_test_case_params(test_case, self._required_params, self)
        self.evaluation_cost = 0 if self.using_native_model else None

        with metric_progress_indicator(self, async_mode=True, _show_indicator=_show_indicator, _in_component=_in_component):
            prompt = self.evaluation_template.generate_score(
                test_case.input,
                test_case.expected_output,
                test_case.retrieval_context
            )

            if self.using_native_model:
                res, cost = await self.model.a_generate(prompt, schema=ContextualPrecisionScore)
                self.evaluation_cost += cost
                self.score = res.score
                self.reason = res.reason
            else:
                try:
                    res: ContextualPrecisionScore = await self.model.a_generate(prompt, schema=ContextualPrecisionScore)
                    self.score = res.score
                    self.reason = res.reason
                except TypeError:
                    res = await self.model.a_generate(prompt)
                    data = trimAndLoadJson(res, self)
                    self.score = data["score"]
                    self.reason = data["reason"]

            self.success = self.score >= self.threshold
            self.verbose_logs = construct_verbose_logs(
                self,
                steps=[f"Score: {self.score}\nReason: {self.reason}"]
            )
            return self.score
