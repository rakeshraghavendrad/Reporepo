return f"""Given the input, expected output, and retrieval context, evaluate contextual precision.

**Instructions:**
- Return JSON with:
  - `score`: float between 0 and 1 (e.g., 0.25, 0.5, 0.75, 1.0) representing how well the retrieval context supports the expected output.
  - `reason`: concise explanation.

**Scoring Guidelines:**
- 1.0 = All context nodes are highly relevant and directly support the expected output.
- 0.75 = Most context nodes are relevant; minor irrelevancies present.
- 0.5 = Mixed relevant and irrelevant nodes; some important details missing.
- 0.25 = Few relevant nodes; major information missing.
- 0.0 = No relevant nodes.

Input:
{input}

Expected Output:
{expected_output}

Retrieval Context{document_count_str}:
{retrieval_context}

Example JSON:
{{
  "score": 0.75,
  "reason": "Most nodes are relevant, but one unrelated context lowers precision."
}}

JSON:
"""
